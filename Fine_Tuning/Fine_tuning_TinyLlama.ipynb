{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fine-tune TinyLlama-1.1B-Chat-v1.0 relatively small model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use `QLoRA (Efficient Finetuning of Quantized LLMs)`, a highly efficient fine-tuning technique that involves `quantizing` a pretrained LLM to just `4` bits and adding small `‚ÄúLow-Rank Adapters‚Äù`. This unique approach allows for fine-tuning LLMs using just a single GPU! This technique is supported by the `PEFT` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step-1 - Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are `not` going to track our training metrics, so let‚Äôs disable Weights and Biases. The `W&B` Platform constitutes a fundamental collection of robust components for monitoring, visualizing data and models, and conveying the results. To deactivate Weights and Biases during the fine-tuning process, set the below environment property.\n",
    "\n",
    "If you have an account with Weights and Biases, feel free to enable it and experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# disable Weights and Biases\n",
    "os.environ['WANDB_DISABLED']=\"true\"\n",
    "os.environ['HUGGING_FACE_API_TOKEN']=\"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Bitsandbytes`: An excellent package that provides a lightweight wrapper around custom CUDA functions that make LLMs go faster ‚Äî optimizers, matrix multiplication, and quantization. In this tutorial, we‚Äôll be using this library to load our model as efficiently as possible.\n",
    "- `transformers`: A library by Hugging Face (ü§ó) that provides pre-trained models and training utilities for various natural language processing tasks.\n",
    "- `peft`: A library by Hugging Face (ü§ó) that enables parameter-efficient fine-tuning.\n",
    "- `accelerate`: Accelerate abstracts exactly and only the boilerplate code related to multi-GPUs/TPU/fp16 and leave the rest of your code unchanged.\n",
    "- `datasets`: Another library by Hugging Face (ü§ó) that provides easy access to a wide range of datasets.\n",
    "- `einops`: A library that simplifies tensor operations.\n",
    "\n",
    "\n",
    "Loading the required libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step-2 - Load the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ubuntu/apps/miniconda3/envs/test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to Load the Datasets, There are Numerous datasets are available for fine-tuning the model. Now we will utilize the DialogSum DataSet from HuggingFace for the fine-tuning process. DialogSum is an extensive dialogue summarization dataset, featuring 13,460 dialogues along with manually labeled summaries and topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step-3 - Load the Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"g3lu/addictive_manufacturing_reasoning\" # https://huggingface.co/datasets/g3lu/addictive_manufacturing_reasoning\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'reason', 'answer'],\n",
      "        num_rows: 9000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'reason', 'answer'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "<class 'dict'> {'question': 'How do the specific requirements for strength, wear resistance, and surface quality in injection mould inserts influence the design and production of additive manufactured tools, and what are the potential trade-offs between these factors in achieving optimal tool performance?', 'reason': \"Okay, so I'm trying to figure out how the specific requirements for strength, wear resistance, and surface quality in injection mold inserts influence the design and production of additive manufactured tools. And also, what are the potential trade-offs between these factors when aiming for optimal tool performance.\\n\\nFirst, let me break down each factor:\\n\\n1. Strength: The insert needs to be strong enough to handle the forces during injection molding without breaking or deforming. So, materials with high strength would be important here.\\n\\n2. Wear Resistance: Over time, as the mold is used repeatedly, it should not wear out too quickly. This means the material should have good abrasion resistance and maybe some hardness.\\n\\n3. Surface Quality: The surface of the insert affects how the molded part looks‚Äîsmooth surfaces mean better product finish, which might be important for aesthetics or functionality.\\n\\nNow, in traditional manufacturing, these inserts are often made using materials like steel because they're strong and wear-resistant. But with additive manufacturing (AM), things might be different because AM allows more complex designs but has its own limitations.\\n\\nSo, when designing an insert with AM:\\n\\n- **Material Selection**: Maybe the material used in AM isn't as strong or wear-resistant as traditional steels? Or perhaps it's a different kind of metal powder. The properties of the AM material will directly impact strength and wear resistance.\\n\\n- **Design Complexity**: AM can make more intricate designs, which might help with cooling channels, for example, making the mold more efficient. But could complex geometries affect strength or surface quality?\\n\\n- **Surface Finish**: I know that AM parts often have a rough surface finish compared to machining. So, after printing, you might need post-processing like polishing to get a smooth surface. That adds time and cost.\\n\\nNow, thinking about trade-offs:\\n\\nIf you prioritize strength, maybe the design becomes bulkier, which could affect wear resistance or surface quality if it's harder to polish a bulky part.\\n\\nIf wear resistance is key, perhaps using a harder material that's less ductile might reduce its overall strength. Or, the process of making it wear-resistant (like adding coatings) might make the production more complex or expensive.\\n\\nSurface quality might require additional steps after AM, like polishing, which takes time and could introduce costs. Alternatively, maybe you can design the mold with features that don't need as high a surface finish, but then the product's appearance suffers.\\n\\nAlso, in AM, the way layers are built up can affect mechanical properties. For example, if the layers aren't fused properly, strength might be lower than expected. But making sure they're well-fused could improve strength and wear resistance, though it might require more precise control during printing, which could slow down production.\\n\\nAnother thought: maybe using a different AM technique affects these factors differently. Like, DMLS (Direct Metal Laser Sintering) vs. Binder Jetting. Each has its own strengths in terms of material properties and surface finish.\\n\\nSo, putting this all together, the design needs to balance these three aspects. If one is prioritized, others might suffer unless compensated for with additional processes or material choices. But those compensations could lead to trade-offs in cost, production time, durability, etc.\\n\\nI'm a bit unsure about how exactly AM affects each factor compared to traditional methods and what specific materials are commonly used. Maybe looking into case studies or existing research would clarify that. Also, the role of post-processing steps is crucial here because it can mitigate some limitations of AM but adds complexity.\", 'answer': \"The design and production of injection mold inserts using additive manufacturing (AM) involve balancing strength, wear resistance, and surface quality, each influenced by material selection, design complexity, and surface finish considerations.\\n\\n1. **Material Selection**:\\n   - **Strength**: Materials with high strength are crucial to withstand forces during molding. AM materials may differ from traditional steels, potentially offering unique properties that could enhance or limit strength.\\n   - **Wear Resistance**: The material's ability to resist abrasion is vital for longevity. Harder materials might be considered but could compromise ductility and overall strength.\\n\\n2. **Design Complexity**:\\n   - AM allows intricate designs, such as efficient cooling channels, which can improve mold efficiency. However, complex geometries might affect structural integrity or surface finish quality.\\n\\n3. **Surface Finish**:\\n   - AM often results in rough surfaces, necessitating post-processing like polishing for a smooth finish. This adds cost and time but is crucial for product aesthetics and functionality.\\n\\n4. **Trade-offs**:\\n   - Prioritizing strength may lead to bulkier designs, potentially affecting wear resistance or surface quality.\\n   - Enhancing wear resistance might require harder materials that are less ductile, impacting overall strength.\\n   - Achieving high surface quality through post-processing increases costs and time but is essential for product finish.\\n\\n5. **AM Techniques**:\\n   - Different AM methods (e.g., DMLS vs. Binder Jetting) offer varying benefits in material properties and surface finish, influencing the balance of the three factors.\\n\\n6. **Post-Processing**:\\n   - Essential for mitigating AM limitations but adds complexity, cost, and time to production processes.\\n\\nIn conclusion, while AM offers design flexibility and potential for optimized molds, achieving optimal performance requires careful consideration of material properties, design, and post-processing steps, each presenting trade-offs that must be balanced against project requirements.\"}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(type(dataset[\"train\"][0]), dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'reason', 'answer'],\n",
      "        num_rows: 8100\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'reason', 'answer'],\n",
      "        num_rows: 900\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# As there is No default Validation set from the Huggingface Dataset, \n",
    "# Now we are splitting the dataset into training set(90%) and validation ser(10%)\n",
    "# With validation we can make the model, not too Overfit\n",
    "# Now, let's split the dataset into 90% training and 10% validation\n",
    "\n",
    "shuffled_dataset = dataset[\"train\"].shuffle(seed=42) # Why? If the data is ordered by topic, difficulty, or other hidden biases ‚Äî that can mess with generalization.\n",
    "split_dataset = shuffled_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "\n",
    "# Now, split_dataset contains the \"train\" and \"test\" split\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# Optional: Re-wrap into DatasetDict if you want to return the result as a dictionary\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": eval_dataset\n",
    "})\n",
    "\n",
    "# Print the new split dataset to confirm\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains the below fields.\n",
    "\n",
    "- question : Question about industrial Implementation.\n",
    "- reason : Reason How to resolve user query step by step.\n",
    "- answer : human written Answer for the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step-4 - Create Bitsandbytes configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using `BitsAndBytesConfig` to load our model in 4-bit format. This will reduce memory consumption considerably, at a cost of some accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step-5 - Loading the Pre-Trained model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TinyLlama`, a Small Language Model(SLM) with 1+ billion parameters.\n",
    "\n",
    "load TinyLlama using `4-bit quantization` from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ubuntu/apps/miniconda3/envs/test/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name='TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "device_map = {\"\": 0}\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is loaded in 4-bit using the `BitsAndBytesConfig` from the bitsandbytes library. This is a part of the QLoRA process, which involves quantizing the pre-trained weights of the model to 4-bit and keeping them fixed during fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step-6 -  Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "incorporating left-padding to optimize memory usage during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "GPU memory occupied: 6911 MB.\n"
     ]
    }
   ],
   "source": [
    "print(original_model.device)  # Should say 'cuda:0'\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "\n",
    "def gen(model,p, maxlen=100, sample=True):\n",
    "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
    "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n",
    "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Explain additive manufacturing.']\n"
     ]
    }
   ],
   "source": [
    "print(gen(original_model, \"Explain additive manufacturing.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step-7 - Test the Model with Zero Shot Inferencing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the base model that we loaded above using a few sample inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruct: Summarize the following conversation.\n",
      "What specific material properties and microstructural characteristics must be optimized and controlled during the development of novel thermoplastic filaments for Material Extrusion (MEX) in order to achieve tailored mechanical performance, thermal stability, and processability, considering factors such as crystallinity, molecular weight, and additive formulations?\n",
      "Output:\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN REASONING:\n",
      "Okay, so I need to figure out what specific material properties and microstructural characteristics are important when developing new thermoplastic filaments for Material Extrusion (MEX). The goal is to get tailored mechanical performance, thermal stability, and processability. Factors like crystallinity, molecular weight, and additive formulations are mentioned.\n",
      "\n",
      "Hmm, let's start by understanding the basics of MEX. It's an additive manufacturing process where thermoplastic filament is melted and extruded layer by layer. So the filament needs to perform well during this process and in the final product.\n",
      "\n",
      "First, mechanical performance. That probably includes strength, flexibility, durability, and maybe something about how it withstands stress over time. For the mechanical properties, I think factors like tensile strength and elongation at break are important. Also, impact resistance is crucial because materials might need to absorb energy without breaking.\n",
      "\n",
      "Now, thermal stability relates to how well the material retains its properties under heat. High melting points would be good for applications where high temperatures are involved. Thermal conductivity could affect how quickly parts cool down after printing, which impacts processing time and part accuracy.\n",
      "\n",
      "Processability refers to how easily the filament can be used in MEX. Melt flow rate is probably a big factor here because it affects how smoothly the material extrudes. Viscosity during melting needs to be just right‚Äînot too high that it's hard to push through the nozzle, but not so low that it doesn't hold its shape after extrusion.\n",
      "\n",
      "Looking at crystallinity: higher crystallinity can make materials stronger and stiffer but might reduce flexibility. It also affects thermal properties because crystalline regions have higher melting points. So balancing crystallinity is important for both mechanical and thermal aspects.\n",
      "\n",
      "Molecular weight is another factor. Higher molecular weight usually means better mechanical strength because the polymer chains are longer and more entangled, which makes the material tougher. But if it's too high, the viscosity might be too great, causing processing issues. So there's a balance needed between molecular weight for strength and processability.\n",
      "\n",
      "Additive formulations include things like fillers, plasticizers, stabilizers, etc. Fillers can add reinforcement or change thermal properties. Plasticizers make materials more flexible but might lower their melting points. Stabilizers help with shelf life and resistance to degradation during processing or use.\n",
      "\n",
      "Microstructural characteristics must also be considered. The size and distribution of particles in the filament affect mechanical properties. If fillers are too big, they might cause weak spots. Even distribution ensures consistent performance across the part. Morphology refers to whether the structure is crystalline, amorphous, or a mix. Processing techniques like extrusion can influence this.\n",
      "\n",
      "Viscoelastic behavior is about how the material responds to stress and strain over time. This affects both how it's processed and how it performs in the final product, especially under dynamic loads.\n",
      "\n",
      "I think I'm missing something about surface quality. The filament needs to have good surface finish for the printed parts, so smooth extrusion without defects like strings or blobs is important. Also, layer adhesion matters; each layer should stick well to form a strong part.\n",
      "\n",
      "What else? Oh, dimensional stability. After printing, the material shouldn't warp or shrink too much, which can cause misalignment of layers and weak spots. This relates to thermal expansion coefficients and cooling rates during processing.\n",
      "\n",
      "So putting it all together: I need to consider crystallinity for strength and thermal properties, molecular weight for strength vs. processability, additive formulations to tune specific properties, microstructural features like particle size and distribution, viscoelastic behavior, surface quality, layer adhesion, and dimensional stability.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "HUMAN FINAL ANSWER:\n",
      "To develop novel thermoplastic filaments for Material Extrusion (MEX) that achieve tailored mechanical performance, thermal stability, and processability, the following material properties and microstructural characteristics must be optimized:\n",
      "\n",
      "1. **Mechanical Performance:**\n",
      "   - **Tensile Strength:** Ensures the filament can withstand applied forces without breaking.\n",
      "   - **Elongation at Break:** Measures flexibility and ductility.\n",
      "   - **Impact Resistance:** Absorbs energy to prevent fracture under stress.\n",
      "\n",
      "2. **Thermal Stability:**\n",
      "   - **High Melting Point:** Maintains structural integrity under high temperatures.\n",
      "   - **Thermal Conductivity:** Affects cooling rate, influencing processing time and accuracy.\n",
      "\n",
      "3. **Processability:**\n",
      "   - **Melt Flow Rate:** Balances extrusion ease without sacrificing shape retention.\n",
      "   - **Viscosity:** Optimal to ensure smooth extrusion without blockages.\n",
      "\n",
      "4. **Crystallinity:**\n",
      "   - Balance between crystalline regions for strength/stiffness and amorphous regions for flexibility, impacting thermal properties.\n",
      "\n",
      "5. **Molecular Weight:**\n",
      "   - High molecular weight enhances strength but must be balanced with processability to avoid excessive viscosity.\n",
      "\n",
      "6. **Additive Formulations:**\n",
      "   - **Fillers:** Reinforce or alter thermal properties.\n",
      "   - **Plasticizers:** Increase flexibility while potentially lowering melting points.\n",
      "   - **Stabilizers:** Enhance durability and shelf life.\n",
      "\n",
      "7. **Microstructural Characteristics:**\n",
      "   - **Particle Size/Distribution:** Ensures consistent performance without weak spots.\n",
      "   - **Morphology:** Crystalline vs. amorphous structure affects properties; influenced by processing techniques.\n",
      "\n",
      "8. **Viscoelastic Behavior:**\n",
      "   - Response to stress/strain over time, impacting both processing and final product performance under dynamic loads.\n",
      "\n",
      "9. **Surface Quality:**\n",
      "   - Smooth extrusion without defects like strings or blobs ensures high-quality printed parts.\n",
      "\n",
      "10. **Layer Adhesion:**\n",
      "    - Strong adhesion between layers is crucial for overall part strength and integrity.\n",
      "\n",
      "11. **Dimensional Stability:**\n",
      "    - Minimizes warping or shrinkage post-printing, ensuring accurate alignment of layers.\n",
      "\n",
      "By carefully balancing these factors, the filament will meet desired mechanical, thermal, and processing requirements, enabling high-performance MEX applications.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "- A detailed analysis of the material properties and microstructural characteristics of the thermoplastic filaments, including their impact on mechanical performance, thermal stability, and processability.\n",
      "- A discussion of the factors that influence the optimization and control of these properties during the development of novel MEMS filaments.\n",
      "- A recommendation for the most suitable additive formulations and their optimal ratios for achieving tailored mechanical performance.\n",
      "- A comparison of the performance of the optimized MEMS\n",
      "CPU times: user 2.84 s, sys: 0 ns, total: 2.84 s\n",
      "Wall time: 2.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "prompt = dataset['train'][index]['question']\n",
    "reason = dataset['train'][index]['reason']\n",
    "answer = dataset['train'][index]['answer']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model, formatted_prompt, 100)\n",
    "\n",
    "output = res[0].split('Output:\\n')[1] if 'Output:' in res[0] else res[0]\n",
    "dash_line = '-' * 100\n",
    "\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN REASONING:\\n{reason}\\n')\n",
    "print(dash_line)\n",
    "print(f'HUMAN FINAL ANSWER:\\n{answer}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the observation above, it‚Äôs evident that the model faces challenges in summarizing the dialogue compared to the baseline summary. However, it manages to extract essential information from the text, suggesting the potential for fine-tuning the model for the specific task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step-8 - Pre-processing dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset cannot be directly employed for fine-tuning. It is essential to format the prompt in a way that the model can comprehend. Referring to the HuggingFace model documentation, it is evident that a prompt needs to be generated using dialogue and summary in the specified format below.\n",
    "\n",
    "\n",
    "To Encourage the model to write more concise answers, we can try the following `QA` Prompt. using \"Instruct:<prompt>\\nOutput:\"\n",
    "\n",
    "Ex:   \n",
    "Instruct: Write a brief analogy between books and windows.  \n",
    "Output: Books are like windows‚Äîeach one opens to a new world, offering a glimpse into lives, ideas, and places beyond our own.  \n",
    "\n",
    "Where the model Generates the Text after \"Output:\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create some helper function to format our input dataset, ensuring it's suitability for fine-tuning process. Here, we need to convert the dialog-summary (prompt response) pairs into explicit instructions to LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats_with_answer(sample):\n",
    "    \"\"\"\n",
    "    Format the fields of the sample, including 'question', 'reason', and 'answer'.\n",
    "    This version includes the answer as part of the expected output for the model to generate.\n",
    "    \n",
    "    :param sample: Sample dictionary\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation and answer the question.\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"Question: {sample['question']}\" if sample.get('question') else None\n",
    "    reason = f\"Reason: {sample['reason']}\" if sample.get('reason') else None\n",
    "    answer = f\"Answer: {sample['answer']}\" if sample.get('answer') else None\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    # Combine the parts into a complete prompt\n",
    "    parts = [part for part in [blurb, instruction, input_context, reason, answer, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    \n",
    "    # Store the formatted prompt in the sample under the 'text' field\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function can be used to convert our input into prompt format.\n",
    "\n",
    "Now, we will use our model tokenizer to process these prompts into tokenized ones.\n",
    "\n",
    "Our aim here is to generate input sequences with consistent lengths, which is beneficial for fine-tuning the language model by optimizing efficiency and minimizing computational overhead. It is essential to ensure that these sequences do not surpass the model‚Äôs maximum token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats_with_answer)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['question', 'reason', 'answer'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By utilizing these functions, our dataset will be prepared for the fine-tuning process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 2048\n",
      "2048\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8100/8100 [00:01<00:00, 4255.37 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8100/8100 [01:08<00:00, 118.20 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8100/8100 [00:09<00:00, 832.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## Pre-process dataset\n",
    "max_length = get_max_length(original_model)\n",
    "print(max_length)\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (7728, 3)\n",
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 7728\n",
      "})\n",
      "GPU memory occupied: 1773 MB.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {train_dataset.shape}\")\n",
    "print(train_dataset)\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step-9 - Setup the PEFT/LoRA model for Fine-Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's perform `Parameter Efficient Fine-Tuning (PEFT)` fine-tuning. `PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning`. PEFT is a generic term that includes Low-Rank Adaptation (LoRA) and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, in essence, enables efficient model fine-tuning using fewer computational resources, often achievable with just a single GPU. Following LoRA fine-tuning for a specific task or use case, the outcome is an unchanged original LLM and the emergence of a considerably smaller \"LoRA adapter,\" often representing a single-digit percentage of the original LLM size (in MBs rather than GBs).\n",
    "\n",
    "During inference, the LoRA adapter must be combined with its original LLM. The advantage lies in the ability of many LoRA adapters to reuse the original LLM, thereby reducing overall memory requirements when handling multiple tasks and use cases.\n",
    "\n",
    "`Note the rank (r) hyper-parameter, which defines the rank/dimension of the adapter to be trained`. `r is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained`. A `higher rank will allow for more expressivity`, but there is a compute tradeoff.\n",
    "\n",
    "alpha is the scaling factor for the learned weights. The weight matrix is scaled by alpha/r, and thus a higher value for alpha assigns more weight to the LoRA activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 131164160\n",
      "all model parameters: 615606272\n",
      "percentage of trainable model parameters: 21.31%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "\n",
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything is set up and the base model is prepared, we can use the print_trainable_parameters() helper function to see how many trainable parameters are in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 6127616\n",
      "all model parameters: 621733888\n",
      "percentage of trainable model parameters: 0.99%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 2048)\n",
      "        (layers): ModuleList(\n",
      "          (0-21): 22 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# See how the model looks different now, with the LoRA adapters added:\n",
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step-10 - Train PEFT Adapters**\n",
    "\n",
    "Define Training arguments and create Trainer instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./peft-tinyLlama-manufacturing-training-{str(int(time.time()))}'\n",
    "\n",
    "import transformers\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=25,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_training_args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 40:16, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.816400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.749100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.705700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.686600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.691500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.664900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.647500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.664700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.643500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.637300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.621300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.626300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.605200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.600300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.616300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.594800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.609600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.614700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.588200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.606800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.588900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.594200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.578700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.595600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.599100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.576900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.592500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.586400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.592900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.6255666465759278, metrics={'train_runtime': 2419.9414, 'train_samples_per_second': 3.306, 'train_steps_per_second': 0.413, 'total_flos': 7.433320241664e+16, 'train_loss': 0.6255666465759278, 'epoch': 1.0351966873706004})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 4001 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory for merging weights\n",
    "del original_model\n",
    "del peft_trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 2355 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained successfully, we can use it for inference. Let‚Äôs now prepare the inference model by adding an adapter to the original `TinyLlama` model. Here, we are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ubuntu/apps/miniconda3/envs/test/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"./peft-tinyLlama-manufacturing-training-1744528743/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step-11 - Human Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " let‚Äôs perform inference using the same input but with the PEFT model, as we did previously in step 7 with the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruct: Summarize the below conversation.\n",
      "\n",
      "What specific material properties and microstructural characteristics must be optimized and controlled during the development of novel thermoplastic filaments for Material Extrusion (MEX) in order to achieve tailored mechanical performance, thermal stability, and processability, considering factors such as crystallinity, molecular weight, and additive formulations?\n",
      "\n",
      "### Output:\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Okay, so I need to figure out what specific material properties and microstructural characteristics are important when developing new thermoplastic filaments for Material Extrusion (MEX). The goal is to get tailored mechanical performance, thermal stability, and processability. Factors like crystallinity, molecular weight, and additive formulations are mentioned.\n",
      "\n",
      "Hmm, let's start by understanding the basics of MEX. It's an additive manufacturing process where thermoplastic filament is melted and extruded layer by layer. So the filament needs to perform well during this process and in the final product.\n",
      "\n",
      "First, mechanical performance. That probably includes strength, flexibility, durability, and maybe something about how it withstands stress over time. For the mechanical properties, I think factors like tensile strength and elongation at break are important. Also, impact resistance is crucial because materials might need to absorb energy without breaking.\n",
      "\n",
      "Now, thermal stability relates to how well the material retains its properties under heat. High melting points would be good for applications where high temperatures are involved. Thermal conductivity could affect how quickly parts cool down after printing, which impacts processing time and part accuracy.\n",
      "\n",
      "Processability refers to how easily the filament can be used in MEX. Melt flow rate is probably a big factor here because it affects how smoothly the material extrudes. Viscosity during melting needs to be just right‚Äînot too high that it's hard to push through the nozzle, but not so low that it doesn't hold its shape after extrusion.\n",
      "\n",
      "Looking at crystallinity: higher crystallinity can make materials stronger and stiffer but might reduce flexibility. It also affects thermal properties because crystalline regions have higher melting points. So balancing crystallinity is important for both mechanical and thermal aspects.\n",
      "\n",
      "Molecular weight is another factor. Higher molecular weight usually means better mechanical strength because the polymer chains are longer and more entangled, which makes the material tougher. But if it's too high, the viscosity might be too great, causing processing issues. So there's a balance needed between molecular weight for strength and processability.\n",
      "\n",
      "Additive formulations include things like fillers, plasticizers, stabilizers, etc. Fillers can add reinforcement or change thermal properties. Plasticizers make materials more flexible but might lower their melting points. Stabilizers help with shelf life and resistance to degradation during processing or use.\n",
      "\n",
      "Microstructural characteristics must also be considered. The size and distribution of particles in the filament affect mechanical properties. If fillers are too big, they might cause weak spots. Even distribution ensures consistent performance across the part. Morphology refers to whether the structure is crystalline, amorphous, or a mix. Processing techniques like extrusion can influence this.\n",
      "\n",
      "Viscoelastic behavior is about how the material responds to stress and strain over time. This affects both how it's processed and how it performs in the final product, especially under dynamic loads.\n",
      "\n",
      "I think I'm missing something about surface quality. The filament needs to have good surface finish for the printed parts, so smooth extrusion without defects like strings or blobs is important. Also, layer adhesion matters; each layer should stick well to form a strong part.\n",
      "\n",
      "What else? Oh, dimensional stability. After printing, the material shouldn't warp or shrink too much, which can cause misalignment of layers and weak spots. This relates to thermal expansion coefficients and cooling rates during processing.\n",
      "\n",
      "So putting it all together: I need to consider crystallinity for strength and thermal properties, molecular weight for strength vs. processability, additive formulations to tune specific properties, microstructural features like particle size and distribution, viscoelastic behavior, surface quality, layer adhesion, and dimensional stability.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL:\n",
      "1. **Optimization of Material Properties:**\n",
      "   - **Crystallinity:** This refers to the degree of crystallinity in the thermoplastic filament. Higher crystallinity can lead to better mechanical properties, such as higher strength and toughness.\n",
      "   - **Molecular Weight:** The molecular weight of the polymer affects its viscosity and thermal stability. Higher molecular weight means a more viscous material, which can be difficult to extrude.\n",
      "   - **Additive Formulations:** The addition of additives can improve mechanical properties by modifying the crystallinity, molecular weight, or other properties. For example, using a higher molecular weight additive can increase strength without increasing viscosity.\n",
      "\n",
      "2. **Microstructural Characteristics:**\n",
      "   - **Microstructure:** The structure of the filament affects its mechanical properties. A homogeneous microstructure can lead to better thermal stability and processability.\n",
      "   - **Density:** The density of the filament affects its flow behavior during extrusion. Higher density can improve flow properties but may reduce mechanical strength.\n",
      "   - **Glass Transition Temperature (Tg):** The glass transition temperature determines how the polymer changes from a liquid to a solid state. Higher Tg can improve mechanical properties but may lead to poor flow properties.\n",
      "\n",
      "3. **Control of Additive Formulations:**\n",
      "   - **Molecular Weight:** The molecular weight of the additives can affect their effectiveness. Higher molecular weight additives can improve strength without increasing viscosity.\n",
      "   - **Viscosity:** The viscosity of the filament affects its flow behavior during extrusion. Higher viscosity can improve mechanical properties but may lead to poor flow properties.\n",
      "   - **Thermal Stability:** The addition of additives can improve thermal stability, especially in cases where the polymer is subjected to high temperatures.\n",
      "\n",
      "4. **Optimization of Microstructure:**\n",
      "   - **Microstructure Optimization:** The microstructure of the filament can be optimized by controlling the crystallinity, molecular weight, and additive formulations.\n",
      "   -\n",
      "CPU times: user 26.5 s, sys: 23.6 ms, total: 26.5 s\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "dialogue = dataset['train'][index]['question']\n",
    "summary = dataset['train'][index]['reason']\n",
    "\n",
    "# Match training-time formatting exactly\n",
    "prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruct: Summarize the below conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "### Output:\n",
    "\"\"\"\n",
    "\n",
    "# Generate with longer max tokens to avoid cutoff\n",
    "peft_model_res = gen(ft_model, prompt, maxlen=500)\n",
    "\n",
    "# Extract model output cleanly\n",
    "# Handle case where \"### End\" might not be present\n",
    "generated_text = peft_model_res[0]\n",
    "if \"### Output:\" in generated_text:\n",
    "    generated_text = generated_text.split(\"### Output:\")[1]\n",
    "summary_output, _, _ = generated_text.partition(\"### End\")\n",
    "\n",
    "# Display nicely\n",
    "dash_line = '-' * 100\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary.strip()}\\n')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{summary_output.strip()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning is often an iterative process. Based on the validation and test sets results, we may need to make further adjustments to the model‚Äôs architecture, hyperparameters, or training data to improve its performance. Let‚Äôs now see how to evaluate the results of Fine-tuned LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step-12 - Evaluate the Model Quantitatively (with ROUGE Metric)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ROUGE, or Recall-Oriented Understudy for Gisting Evaluation`, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\n",
    "\n",
    "\n",
    "Let‚Äôs now use the `ROUGE metric` to quantify the validity of summarizations produced by models. It compares summarizations to a ‚Äúbaseline‚Äù summary which is usually created by a human. While it‚Äôs not a perfect metric, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning.\n",
    "\n",
    "To demonstrate the capability of ROUGE Metric Evaluation we will use some sample inputs to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ubuntu/apps/miniconda3/envs/test/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Summarize the conversation on cooperative agreements between standardization bodies, such as the PSDO agreement between ASTM International and ISO, and their impact on harmonizing national standards and technical regulations in additive manufacturing.\n",
      "- Discuss the potential benefits and challenges of implementing a three-tier structure of AM standards across different categories and classes, considering factors such as the complexity of the standards, the role of international organizations, and the need for standardization bodies\n",
      "- Summarize the conversation about additive manufacturing optimization.\n",
      "- Explain how the additive manufacturing process can be optimized to minimize errors and difficulties in product development, particularly in terms of identifying and addressing underlying issues early on.\n",
      "- Discuss the role of iterative design, automated processes, and computer-aided design in facilitating this optimization.\n",
      "- Provide examples of how these strategies have been successfully applied in different industries.\n",
      "\n",
      "\n",
      "- Design considerations:\n",
      "  - 1. **Accuracy**: Ensure that the model accurately represents the patient's anatomy, including any complex structures or variations in shape.\n",
      "  - 2. **Reproducibility**: The model must be easily reproducible, with consistent features and dimensions across different manufacturing processes.\n",
      "  - 3. **Integration with existing systems**: The model must seamlessly integrate with medical imaging\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dialogues = dataset['train'][0:3]['question']\n",
    "human_baseline_summaries = dataset['train'][0:10]['reason']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
    "    \n",
    "    original_model_res = gen(original_model,prompt,100,)\n",
    "    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n",
    "    \n",
    "    peft_model_res = gen(ft_model,prompt,100,)\n",
    "    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "    print(peft_model_output)\n",
    "    peft_model_text_output, success, result = peft_model_output.partition('###')\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n",
    "df\n",
    "\n",
    "df.to_csv(\"model_summary_comparison.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.1339806559953702, 'rouge2': 0.06260404490693139, 'rougeL': 0.09707812571423212, 'rougeLsum': 0.12751964964755005}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.149750761876448, 'rouge2': 0.06631024046241864, 'rougeL': 0.10370722351347923, 'rougeLsum': 0.14226401745278225}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric--------Base Model--PEFT Model--üî• Improvement  \n",
    "ROUGE-1-------0.134-------0.150-------‚úÖ (more overlap of unigrams)  \n",
    "ROUGE-2-------0.063-------0.066-------‚úÖ (slight nudge in bigram quality)  \n",
    "ROUGE-L-------0.097-------0.104-------‚úÖ (better longest common subsequence match)  \n",
    "ROUGE-Lsum---0.128-------0.142-------‚úÖ (summary-level coherence)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE scores, commonly used metrics for evaluating text summarization and natural language generation (NLG) tasks. ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It compares the model's output (generated summary) with a reference (human-written) summary, measuring how much overlap there is in terms of words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n",
      "rouge1: 1.58%\n",
      "rouge2: 0.37%\n",
      "rougeL: 0.66%\n",
      "rougeLsum: 1.47%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric-----Improvement (%)---üí¨ Interpretation  \n",
    "ROUGE-1----+1.58%------------Better unigram (word) overlap  \n",
    "ROUGE-2----+0.37%------------Slightly better phrase-level match (bigrams)    \n",
    "ROUGE-L----+0.66%------------More structure-aligned summaries  \n",
    "ROUGE-Lsum-+1.47%------------Better summary-level alignment with human ref  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above results, there is a significant improvement in the PEFT model as compared to the original model denoted in terms of percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model BERTScore F1: 0.8351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model BERTScore F1: 0.8394\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Human references\n",
    "references = df['human_baseline_summaries'].tolist()\n",
    "\n",
    "# Model summaries\n",
    "original_preds = df['original_model_summaries'].tolist()\n",
    "peft_preds = df['peft_model_summaries'].tolist()\n",
    "\n",
    "# BERTScore for Original Model\n",
    "P_o, R_o, F1_o = score(original_preds, references, lang=\"en\")\n",
    "print(f\"Original Model BERTScore F1: {F1_o.mean().item():.4f}\")\n",
    "\n",
    "# BERTScore for PEFT Model\n",
    "P_p, R_p, F1_p = score(peft_preds, references, lang=\"en\")\n",
    "print(f\"PEFT Model BERTScore F1: {F1_p.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Perplexity (avg): 5.91\n",
      "PEFT Model Perplexity (avg): 4.70\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Use the same tokenizer/model for both (just need one to get perplexity)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")  # e.g., \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./peft-tinyLlama-manufacturing-training-1744528743/checkpoint-1000\").cuda()\n",
    "\n",
    "def compute_perplexity(text, model, tokenizer):\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = encodings.input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "    return math.exp(loss.item())\n",
    "\n",
    "# Apply to your outputs\n",
    "original_ppl = [compute_perplexity(text, model, tokenizer) for text in original_preds]\n",
    "peft_ppl = [compute_perplexity(text, model, tokenizer) for text in peft_preds]\n",
    "\n",
    "print(f\"Original Model Perplexity (avg): {sum(original_ppl)/len(original_ppl):.2f}\")\n",
    "print(f\"PEFT Model Perplexity (avg): {sum(peft_ppl)/len(peft_ppl):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT BLEU Score (avg, smoothed): 0.0332\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "bleu_scores = [\n",
    "    sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)\n",
    "    for ref, pred in zip(df['human_baseline_summaries'], df['peft_model_summaries'])\n",
    "]\n",
    "\n",
    "print(f\"PEFT BLEU Score (avg, smoothed): {np.mean(bleu_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ubuntu/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional but improves METEOR synonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT METEOR Score (avg): 0.2819\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "meteor_scores = [\n",
    "    meteor_score([ref.split()], pred.split())\n",
    "    for ref, pred in zip(df['human_baseline_summaries'], df['peft_model_summaries'])\n",
    "]\n",
    "\n",
    "print(f\"PEFT METEOR Score (avg): {np.mean(meteor_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
